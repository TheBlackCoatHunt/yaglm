{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the concave penalties that come with `ya_glm`. We fit these penalties using the LLA algorithm applied go a \"good enough\" initializer as in Fan et al. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ya_glm.toy_data import sample_sparse_lin_reg\n",
    "from ya_glm.backends.fista.LinearRegression import FcpLLA, FcpLLACV,\\\n",
    "    LassoCV, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize, then fit concave penalty the LLA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some linear regression data\n",
    "X, y, coef, intercept = sample_sparse_lin_reg(n_samples=100, n_features=10, n_nonzero=5,\n",
    "                                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the coefficient with a Lasso estimate tuned with cross-validation\n",
    "init = LassoCV().fit(X, y)\n",
    "\n",
    "# these will give the same behavior\n",
    "# init = LassoCV() # if an unfit estimator is passed in, it will be fit to the data\n",
    "# init = 'default' # the default will use a LassoCV\n",
    "\n",
    "# fit the concave penalty by initializing from the Lasso estimate\n",
    "concave_est = FcpLLA(init=init)\n",
    "\n",
    "# by default we take one LLA step -- see Fan et al 2014\n",
    "# concave_est = FcpLLA(n_lla_steps=1, init=init) # default behavior\n",
    "# but we can of course run for more LLA steps!\n",
    "concave_est = FcpLLA(lla_n_steps=100, init=init)\n",
    "\n",
    "# the cross-validation estimator takes the same arguments\n",
    "# not we will only fit the initialize once and use\n",
    "# that fit for every fold\n",
    "cv_est = FcpLLACV(lla_n_steps=100, init=LassoCV())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare different estimators\n",
    "\n",
    "Lets compare different initialization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some linear regression data\n",
    "# here lets use higher dimensional data\n",
    "X, y, coef, intercept = sample_sparse_lin_reg(n_samples=100, n_features=100, n_nonzero=5,\n",
    "                                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.62 s, sys: 561 ms, total: 5.18 s\n",
      "Wall time: 1.96 s\n",
      "Ridge L2 to truth 1.46744703053894 \n",
      "\n",
      "CPU times: user 6.51 s, sys: 545 ms, total: 7.05 s\n",
      "Wall time: 3.83 s\n",
      "Lasso L2 to truth 0.5399939910125182 \n",
      "\n",
      "CPU times: user 1min 21s, sys: 13.3 s, total: 1min 35s\n",
      "Wall time: 24.3 s\n",
      "FCP, ridge init, one step L2 to truth 0.3434980952377849 \n",
      "\n",
      "CPU times: user 44.1 s, sys: 6.69 s, total: 50.7 s\n",
      "Wall time: 13 s\n",
      "FCP, lasso init, one step L2 to truth 0.29344737097220097 \n",
      "\n",
      "CPU times: user 2min 59s, sys: 28 s, total: 3min 27s\n",
      "Wall time: 56.2 s\n",
      "FCP, ridge init, many steps L2 to truth 0.29640971321327547 \n",
      "\n",
      "CPU times: user 2min 57s, sys: 26.8 s, total: 3min 24s\n",
      "Wall time: 54.1 s\n",
      "FCP, lasso init, many steps L2 to truth 0.296093645006947 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = 10\n",
    "\n",
    "# Ridge regression\n",
    "%time ridge = RidgeCV(cv=cv).fit(X, y)\n",
    "print('Ridge L2 to truth',\n",
    "      np.linalg.norm(ridge.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# Lasso\n",
    "%time lasso = LassoCV(cv=cv).fit(X, y)\n",
    "print('Lasso L2 to truth',\n",
    "      np.linalg.norm(lasso.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from ridge, one step\n",
    "%time fcp_from_ridge_1 = FcpLLACV(init=ridge, lla_n_steps=1, cv=cv).fit(X, y)\n",
    "print('FCP, ridge init, one step L2 to truth',\n",
    "      np.linalg.norm(fcp_from_ridge_1.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from lasso, one step\n",
    "%time fcp_from_lasso_1 = FcpLLACV(init=lasso, lla_n_steps=1, cv=cv).fit(X, y)\n",
    "print('FCP, lasso init, one step L2 to truth',\n",
    "      np.linalg.norm(fcp_from_lasso_1.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from ridge, many steps\n",
    "%time fcp_from_ridge_many = FcpLLACV(init=ridge, lla_n_steps=100, cv=cv).fit(X, y)\n",
    "print('FCP, ridge init, many steps L2 to truth',\n",
    "      np.linalg.norm(fcp_from_ridge_many.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "\n",
    "%time fcp_from_lasso_many = FcpLLACV(init=lasso, lla_n_steps=100, cv=cv).fit(X, y)\n",
    "print('FCP, lasso init, many steps L2 to truth',\n",
    "      np.linalg.norm(fcp_from_lasso_many.best_estimator_.coef_ - coef), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:repro_lap_reg] *",
   "language": "python",
   "name": "conda-env-repro_lap_reg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
