{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare initialization strategies for the LLA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the concave penalties that come with `ya_glm`. We fit these penalties using the LLA algorithm applied go a \"good enough\" initializer as in Fan et al. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ya_glm.toy_data import sample_sparse_lin_reg\n",
    "\n",
    "\n",
    "from ya_glm.models.Lasso import LassoCV\n",
    "from ya_glm.models.Ridge import RidgeCV\n",
    "from ya_glm.models.ENet import ENetCV\n",
    "\n",
    "from ya_glm.models.FcpLLA import FcpLLA, FcpLLACV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize, then fit concave penalty the LLA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some linear regression data\n",
    "X, y, coef, intercept = sample_sparse_lin_reg(n_samples=100, n_features=10, n_nonzero=5,\n",
    "                                              X_dist='corr', x_corr=.1,\n",
    "                                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the coefficient with a Lasso estimate tuned with cross-validation\n",
    "init = LassoCV().fit(X, y)\n",
    "\n",
    "# these will give the same behavior\n",
    "# init = LassoCV() # if an unfit estimator is passed in, it will be fit to the data\n",
    "# init = 'default' # the default will use a LassoCV\n",
    "\n",
    "# fit the concave penalty by initializing from the Lasso estimate\n",
    "concave_est = FcpLLA(init=init)\n",
    "\n",
    "# by default we take one LLA step -- see Fan et al 2014\n",
    "# concave_est = FcpLLA(n_lla_steps=1, init=init) # default behavior\n",
    "# but we can of course run for more LLA steps!\n",
    "concave_est = FcpLLA(lla_n_steps=100, init=init)\n",
    "\n",
    "# note the cross-validation estimator will fit the initializer once!\n",
    "# then will use the same initailizer for each cross-validation fold\n",
    "cv_est = FcpLLACV(estimator=concave_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare different estimators\n",
    "\n",
    "Lets compare different initialization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some linear regression data\n",
    "# here lets use higher dimensional data\n",
    "X, y, coef, intercept = sample_sparse_lin_reg(n_samples=100, n_features=100, n_nonzero=5,\n",
    "                                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 219 ms, sys: 107 ms, total: 325 ms\n",
      "Wall time: 3.16 s\n",
      "Lasso L2 to truth 0.5399082946992609 \n",
      "\n",
      "CPU times: user 259 ms, sys: 185 ms, total: 444 ms\n",
      "Wall time: 760 ms\n",
      "Ridge L2 to truth 1.4668701386280432 \n",
      "\n",
      "CPU times: user 919 ms, sys: 55.7 ms, total: 974 ms\n",
      "Wall time: 13.2 s\n",
      "ENet L2 to truth 0.5400790718053406 \n",
      "\n",
      "CPU times: user 1.91 s, sys: 158 ms, total: 2.07 s\n",
      "Wall time: 9.7 s\n",
      "FCP, lasso init, one step L2 to truth 0.2934439869808888 \n",
      "\n",
      "CPU times: user 2.39 s, sys: 170 ms, total: 2.56 s\n",
      "Wall time: 15 s\n",
      "FCP, ridge init, one step L2 to truth 0.34341860588566014 \n",
      "\n",
      "CPU times: user 3.05 s, sys: 169 ms, total: 3.22 s\n",
      "Wall time: 27.7 s\n",
      "FCP, enet init, one step L2 to truth 0.29340474485873375 \n",
      "\n",
      "CPU times: user 3.62 s, sys: 283 ms, total: 3.91 s\n",
      "Wall time: 58.6 s\n",
      "FCP, lasso init, many steps L2 to truth 0.2964121594377123 \n",
      "\n",
      "CPU times: user 3.95 s, sys: 353 ms, total: 4.3 s\n",
      "Wall time: 48.2 s\n",
      "FCP, ridge init, many steps L2 to truth 0.2964121594377123 \n",
      "\n",
      "CPU times: user 3.57 s, sys: 254 ms, total: 3.83 s\n",
      "Wall time: 23.1 s\n",
      "FCP, enet init, many steps L2 to truth 0.2964121594377123 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_kws = {'cv': 10, 'cv_n_jobs': -1}\n",
    "\n",
    "# Lasso\n",
    "%time lasso = LassoCV(**cv_kws).fit(X, y)\n",
    "print('Lasso L2 to truth',\n",
    "      np.linalg.norm(lasso.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "\n",
    "# Ridge regression\n",
    "%time ridge = RidgeCV(**cv_kws).fit(X, y)\n",
    "print('Ridge L2 to truth',\n",
    "      np.linalg.norm(ridge.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# ElasticNet\n",
    "%time enet = ENetCV(l1_ratio='tune', **cv_kws).fit(X, y)\n",
    "print('ENet L2 to truth',\n",
    "      np.linalg.norm(enet.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from lasso, one step\n",
    "%time fcp_from_lasso_1 = FcpLLACV(estimator=FcpLLA(init=lasso, lla_n_steps=1),\\\n",
    "                                  **cv_kws).fit(X, y)\n",
    "print('FCP, lasso init, one step L2 to truth',\n",
    "      np.linalg.norm(fcp_from_lasso_1.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from ridge, one step\n",
    "%time fcp_from_ridge_1 = FcpLLACV(estimator=FcpLLA(init=ridge, lla_n_steps=1),\\\n",
    "                                  **cv_kws).fit(X, y)\n",
    "print('FCP, ridge init, one step L2 to truth',\n",
    "      np.linalg.norm(fcp_from_ridge_1.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from enet, one step\n",
    "%time fcp_from_enet_1 = FcpLLACV(estimator=FcpLLA(init=enet, lla_n_steps=1),\\\n",
    "                                 **cv_kws).fit(X, y)\n",
    "print('FCP, enet init, one step L2 to truth',\n",
    "      np.linalg.norm(fcp_from_enet_1.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from lasso, many steps\n",
    "%time fcp_from_lasso_many = FcpLLACV(estimator=FcpLLA(init=ridge, lla_n_steps=100),\\\n",
    "                                     **cv_kws).fit(X, y)\n",
    "print('FCP, lasso init, many steps L2 to truth',\n",
    "      np.linalg.norm(fcp_from_lasso_many.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from ridge, many steps\n",
    "%time fcp_from_ridge_many = FcpLLACV(estimator=FcpLLA(init=ridge, lla_n_steps=100),\\\n",
    "                                     **cv_kws).fit(X, y)\n",
    "print('FCP, ridge init, many steps L2 to truth',\n",
    "      np.linalg.norm(fcp_from_ridge_many.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from enet, many steps\n",
    "%time fcp_from_enet_many = FcpLLACV(estimator=FcpLLA(init=enet, lla_n_steps=1),\\\n",
    "                                    **cv_kws).fit(X, y)\n",
    "print('FCP, enet init, many steps L2 to truth',\n",
    "      np.linalg.norm(fcp_from_ridge_many.best_estimator_.coef_ - coef), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
