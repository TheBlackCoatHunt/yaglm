{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare initialization strategies for the LLA algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the concave penalties that come with `ya_glm`. We fit these penalties using the LLA algorithm applied go a \"good enough\" initializer as in Fan et al. 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ya_glm.toy_data import sample_sparse_lin_reg\n",
    "from ya_glm.backends.fista.LinearRegression import FcpLLA, FcpLLACV,\\\n",
    "    LassoCV, RidgeCV, LassoENetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize, then fit concave penalty the LLA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some linear regression data\n",
    "X, y, coef, intercept = sample_sparse_lin_reg(n_samples=100, n_features=10, n_nonzero=5,\n",
    "                                              X_dist='corr', x_corr=.1,\n",
    "                                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the coefficient with a Lasso estimate tuned with cross-validation\n",
    "init = LassoCV().fit(X, y)\n",
    "\n",
    "# these will give the same behavior\n",
    "# init = LassoCV() # if an unfit estimator is passed in, it will be fit to the data\n",
    "# init = 'default' # the default will use a LassoCV\n",
    "\n",
    "# fit the concave penalty by initializing from the Lasso estimate\n",
    "concave_est = FcpLLA(init=init)\n",
    "\n",
    "# by default we take one LLA step -- see Fan et al 2014\n",
    "# concave_est = FcpLLA(n_lla_steps=1, init=init) # default behavior\n",
    "# but we can of course run for more LLA steps!\n",
    "concave_est = FcpLLA(lla_n_steps=100, init=init)\n",
    "\n",
    "# note the cross-validation estimator will fit the initializer once!\n",
    "# then will use the same initailizer for each cross-validation fold\n",
    "cv_est = FcpLLACV(estimator=concave_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare different estimators\n",
    "\n",
    "Lets compare different initialization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some linear regression data\n",
    "# here lets use higher dimensional data\n",
    "X, y, coef, intercept = sample_sparse_lin_reg(n_samples=100, n_features=100, n_nonzero=5,\n",
    "                                              random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 81.5 ms, total: 233 ms\n",
      "Wall time: 3.7 s\n",
      "Lasso L2 to truth 0.5399115512254988 \n",
      "\n",
      "CPU times: user 246 ms, sys: 84 ms, total: 330 ms\n",
      "Wall time: 1.08 s\n",
      "Ridge L2 to truth 1.4667162260738444 \n",
      "\n",
      "CPU times: user 1.02 s, sys: 77.9 ms, total: 1.1 s\n",
      "Wall time: 18.1 s\n",
      "ENet L2 to truth 0.5399115512254988 \n",
      "\n",
      "CPU times: user 1.92 s, sys: 221 ms, total: 2.14 s\n",
      "Wall time: 6.35 s\n",
      "FCP, lasso init, one step L2 to truth 0.2934270519442212 \n",
      "\n",
      "CPU times: user 2.01 s, sys: 205 ms, total: 2.21 s\n",
      "Wall time: 11.1 s\n",
      "FCP, ridge init, one step L2 to truth 0.3433528089934003 \n",
      "\n",
      "CPU times: user 2.65 s, sys: 196 ms, total: 2.85 s\n",
      "Wall time: 21.1 s\n",
      "FCP, enet init, one step L2 to truth 0.2934270519442212 \n",
      "\n",
      "CPU times: user 2.59 s, sys: 218 ms, total: 2.81 s\n",
      "Wall time: 18.4 s\n",
      "FCP, lasso init, many steps L2 to truth 0.2964122567536647 \n",
      "\n",
      "CPU times: user 2.56 s, sys: 265 ms, total: 2.83 s\n",
      "Wall time: 19.7 s\n",
      "FCP, ridge init, many steps L2 to truth 0.2964122567536647 \n",
      "\n",
      "CPU times: user 2.85 s, sys: 201 ms, total: 3.05 s\n",
      "Wall time: 28.5 s\n",
      "FCP, enet init, many steps L2 to truth 0.2964122567536647 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_kws = {'cv': 10, 'cv_n_jobs': -1}\n",
    "\n",
    "# Lasso\n",
    "%time lasso = LassoCV(**cv_kws).fit(X, y)\n",
    "print('Lasso L2 to truth',\n",
    "      np.linalg.norm(lasso.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "\n",
    "# Ridge regression\n",
    "%time ridge = RidgeCV(**cv_kws).fit(X, y)\n",
    "print('Ridge L2 to truth',\n",
    "      np.linalg.norm(ridge.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# ElasticNet\n",
    "%time enet = LassoENetCV(l1_ratio='tune', **cv_kws).fit(X, y)\n",
    "print('ENet L2 to truth',\n",
    "      np.linalg.norm(enet.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from lasso, one step\n",
    "%time fcp_from_lasso_1 = FcpLLACV(estimator=FcpLLA(init=lasso, lla_n_steps=1),\\\n",
    "                                  **cv_kws).fit(X, y)\n",
    "print('FCP, lasso init, one step L2 to truth',\n",
    "      np.linalg.norm(fcp_from_lasso_1.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from ridge, one step\n",
    "%time fcp_from_ridge_1 = FcpLLACV(estimator=FcpLLA(init=ridge, lla_n_steps=1),\\\n",
    "                                  **cv_kws).fit(X, y)\n",
    "print('FCP, ridge init, one step L2 to truth',\n",
    "      np.linalg.norm(fcp_from_ridge_1.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from enet, one step\n",
    "%time fcp_from_enet_1 = FcpLLACV(estimator=FcpLLA(init=enet, lla_n_steps=1),\\\n",
    "                                 **cv_kws).fit(X, y)\n",
    "print('FCP, enet init, one step L2 to truth',\n",
    "      np.linalg.norm(fcp_from_enet_1.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from lasso, many steps\n",
    "%time fcp_from_lasso_many = FcpLLACV(estimator=FcpLLA(init=ridge, lla_n_steps=100),\\\n",
    "                                     **cv_kws).fit(X, y)\n",
    "print('FCP, lasso init, many steps L2 to truth',\n",
    "      np.linalg.norm(fcp_from_lasso_many.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from ridge, many steps\n",
    "%time fcp_from_ridge_many = FcpLLACV(estimator=FcpLLA(init=ridge, lla_n_steps=100),\\\n",
    "                                     **cv_kws).fit(X, y)\n",
    "print('FCP, ridge init, many steps L2 to truth',\n",
    "      np.linalg.norm(fcp_from_ridge_many.best_estimator_.coef_ - coef), '\\n')\n",
    "\n",
    "# FCP, initialize from enet, many steps\n",
    "%time fcp_from_enet_many = FcpLLACV(estimator=FcpLLA(init=enet, lla_n_steps=1),\\\n",
    "                                    **cv_kws).fit(X, y)\n",
    "print('FCP, enet init, many steps L2 to truth',\n",
    "      np.linalg.norm(fcp_from_ridge_many.best_estimator_.coef_ - coef), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
